import numpy as np
import matplotlib.pyplot as plt
import time

each_run_step = 1000
run_time = 1


class TestBed:
    # the mean value of each action
    mean = [0.2, -0.4, 1.5, 0.3, 1.2, -1.5, -0.1, -1, 0.8, -0.4]

    # accumulate reward, for compare model
    total_reward = 0

    # optimal action count (when action = 2)
    optimal_action = np.zeros(each_run_step)
    optimal_cnt = 0

    # default greedy, moving average and step size is 0.1, init realistic
    def __init__(self, epsilon=0.0, moving=True, init_action_value=0.0, step_size=0.1):
        self.epsilon = epsilon
        self.moving = moving
        self.action_value = np.ones(10) * init_action_value
        self.step_size = step_size

    # action in [0, 9]
    def compute_reward(self, action):
        return np.random.randn() + self.mean[action]

    # update action value after one step.
    # new value = old value + step size * error
    def update_action_value(self, action, value):
        self.action_value[action] += self.step_size * (
                value - self.action_value[action])

    # update optimal action percentage and count
    def update_optimal_action_count(self, action, time_step):
        if action == 2:
            self.optimal_cnt += 1
        self.optimal_action[time_step] = self.optimal_cnt / (time_step + 1)

    # update total reward
    def update_reward(self, reward):
        self.total_reward += reward

    def step(self, times):
        # find action which lead to max value
        greedy_action = self.action_value.argmax()

        # probability epsilon random choose action
        action = greedy_action
        if np.random.random() < self.epsilon:
            # [low, high)
            action = np.random.randint(0, 10)

        # do action, get reward
        reward = self.compute_reward(action)

        # update total reward
        self.update_reward(reward)
        # update the reward function
        self.update_action_value(action, reward)
        # update optimal action count
        self.update_optimal_action_count(action, times)

        return self.total_reward

    def run(self):
        return [self.step(x) / (x + 1) for x in range(each_run_step)]


start_time = time.time()

# init array
reward1 = np.zeros(each_run_step)
reward2 = np.zeros(each_run_step)
op1 = np.zeros(each_run_step)
op2 = np.zeros(each_run_step)
label1 = "realistic, ep = 0.1"
label2 = "optimistic, ep = 00"

for i in range(run_time):
    epsilon_01_0 = TestBed(epsilon=0.1, init_action_value=0)
    reward1 += epsilon_01_0.run()
    op1 += epsilon_01_0.optimal_action

    epsilon_0_5 = TestBed(epsilon=0, init_action_value=5)
    reward2 += epsilon_0_5.run()
    op2 += epsilon_0_5.optimal_action

step = [x + 1 for x in range(each_run_step)]


def show_average_reward():
    plt.xkcd()

    plt.plot(step, reward1 / run_time, label=label1)
    plt.plot(step, reward2 / run_time, label=label2)
    plt.xlabel("time step")
    plt.ylabel("Average reward")
    plt.title(f"10-armed bandit, average on {run_time} runs")

    plt.legend()

    plt.tight_layout()
    # plt.savefig("reward.png")
    plt.show()


def show_optimal_action():
    plt.xkcd()

    plt.plot(step, op1 / run_time, label=label1)
    plt.plot(step, op2 / run_time, label=label2)

    plt.xlabel("time step")
    plt.ylabel("optimal action percentage")
    plt.title(f"10-armed bandit, average on {run_time} runs")

    plt.legend()

    plt.tight_layout()
    plt.show()


show_optimal_action()
# show_average_reward()

# when picture closed, then program stop.
print(f"total time is {time.time() - start_time}")
