import numpy as np
import matplotlib.pyplot as plt
import time

each_run_step = 1000
run_time = 100


class TestBed:

    # default greedy, moving average and step size is 0.1, init realistic
    def __init__(self, epsilon=0.0, epsilon_greedy=True, moving=True, init_action_value=0.0, step_size=0.1,
                 preference_step_size=0.1,
                 ):
        self.epsilon = epsilon
        self.epsilon_greedy = epsilon_greedy
        self.moving = moving
        self.action_value = np.ones(10) * init_action_value
        self.step_size = step_size
        self.preference = np.zeros(10)
        self.preference_step_size = preference_step_size

        # should not know
        self.mean = np.random.randn(10)+4
        self.best_action = self.mean.argmax()

        # count reward and percent of optimal action has been chosen
        self.total_reward = 0
        self.optimal_action = np.zeros(each_run_step)
        self.optimal_cnt = 0

    # action in [0, 9]
    def compute_reward(self, action):
        return np.random.randn() + self.mean[action]

    # update action value after one step.
    # new value = old value + step size * error
    def update_action_value(self, action, value):
        self.action_value[action] += self.step_size * (
                value - self.action_value[action])

    # update optimal action percentage and count
    def update_optimal_action_count(self, action, time_step):
        if action == self.best_action:
            self.optimal_cnt += 1
        self.optimal_action[time_step] = self.optimal_cnt / (time_step + 1)

    # update total reward
    def update_reward(self, reward):
        self.total_reward += reward

    # epsilon_greedy to choose action
    def epsilon_greedy_action(self):
        # find action which lead to max value
        greedy_action = self.action_value.argmax()

        # probability epsilon random choose action
        action = greedy_action
        if np.random.random() < self.epsilon:
            # [low, high)
            action = np.random.randint(0, 10)
        return action

    def softmax(self):
        if min(self.preference) > 0:
            self.preference -= min(self.preference)
        exp = np.exp(self.preference)
        exp_sum = sum(exp)
        probability = exp / exp_sum
        return probability

    def preference_action(self, softmax):
        return np.random.choice(10, 1, p=softmax)

    def update_preference(self, action, reward, probability):
        diff = reward - self.action_value
        probability[action] -= 1
        self.preference -= self.preference_step_size * diff * probability

    def step(self, times):

        action = -1
        probability = np.zeros(10)
        if self.epsilon_greedy:
            action = self.epsilon_greedy_action()
        else:
            probability = self.softmax()
            action = self.preference_action(probability)

        # do action, get reward
        reward = self.compute_reward(action)

        if not self.epsilon_greedy:
            self.update_preference(action, reward, probability)
        # update total reward
        self.update_reward(reward)
        # update the reward function
        self.update_action_value(action, reward)
        # update optimal action count
        self.update_optimal_action_count(action, times)

        return self.total_reward

    def run(self):
        result = np.zeros(each_run_step)
        for x in range(each_run_step):
            result[x] = self.step(x) / (x + 1)
        return result


start_time = time.time()

# init array
reward1 = np.zeros((1, each_run_step))
reward2 = np.zeros(each_run_step)
op1 = np.zeros(each_run_step)
op2 = np.zeros(each_run_step)
label1 = "step 0.1, 0.1"
label2 = "step 0.1, 0.4"

for i in range(run_time):
    epsilon_01_0 = TestBed(epsilon_greedy=False, step_size=0.1)
    res = epsilon_01_0.run()
    # print(f"reward shape {reward1.shape} plus shape {len(res[0])}")
    reward1 += res
    op1 += epsilon_01_0.optimal_action

    epsilon_0_5 = TestBed(epsilon_greedy=False, step_size=0.4, preference_step_size=0.1)
    reward2 += epsilon_0_5.run()
    op2 += epsilon_0_5.optimal_action

step = [x + 1 for x in range(each_run_step)]


def show_average_reward():
    plt.xkcd()

    plt.plot(step, reward1 / run_time, label=label1)
    plt.plot(step, reward2 / run_time, label=label2)
    plt.xlabel("time step")
    plt.ylabel("Average reward")
    plt.title(f"10-armed bandit, average on {run_time} runs")

    plt.legend()

    plt.tight_layout()
    # plt.savefig("reward.png")
    plt.show()


def show_optimal_action():
    plt.xkcd()

    plt.plot(step, op1 / run_time, label=label1)
    plt.plot(step, op2 / run_time, label=label2)

    plt.xlabel("time step")
    plt.ylabel("optimal action percentage")
    plt.title(f"10-armed bandit, average on {run_time} runs")

    plt.legend()

    plt.tight_layout()
    plt.show()


show_optimal_action()
# show_average_reward()

# when picture closed, then program stop.
print(f"total time is {time.time() - start_time}")
